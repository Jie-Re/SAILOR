activation: selu
alpha: 0.5994842503189409
arch: 1
augmentor:
  _lr: 0.1
  hidden: 64
  lr: 0.1
  n_layers: 1
  optimizer: Adam
  weight_decay: 0.0005
batch: 0
batch_size: 0
beta: 0.01
dataset: Pubmed
degthres: 0
drop_prt: 0.3763908980400059
dropout: 0.5
epoch: 2000
eta: 0.01
evaluate_mode: 1
gnn:
  hidden: 64
  lr: 0.1
  n_layers: 1
  norm_type: layerNorm
  optimizer: AMSGrad
  weight_decay: 0.0005
n_trials: 100
patience: 200
split_type: tail
temperature: 1.603671511566729
theta: 0.01
use_lcc: true
use_undirected: true
verbose: 0
visualize: 0
with_bias: 1
with_decay: 1
with_gpu: 1
